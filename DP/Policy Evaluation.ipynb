{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobjun/Python notebooks/reinforcement-learning-exercises/RL-practice/lib/python3.8/site-packages/gym/utils/seeding.py:41: DeprecationWarning: \u001b[33mWARN: Function `rng.rand(*size)` is marked as deprecated and will be removed in the future. Please use `Generator.random(size)` instead.\u001b[0m\n",
      "  max_bytes: Maximum number of bytes to use in the hashed seed.\n"
     ]
    }
   ],
   "source": [
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    # initialize the state space\n",
    "    S = env.nS\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # iterate through each state\n",
    "        for s in range(S):\n",
    "            # Use each available action to update the current value function\n",
    "            new_value = 0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # using the lookahead idea from Bellman expectation equation to calculate the current value\n",
    "                # note that env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    new_value += prob * action_prob * (reward + (discount_factor * V[next_state]))\n",
    "\n",
    "                # update delta value\n",
    "                delta = max(delta, np.abs(new_value - V[s]))\n",
    "                # update the V array \n",
    "                V[s] = new_value\n",
    "\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jacobjun/Python notebooks/reinforcement-learning-exercises/DP/Policy Evaluation.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jacobjun/Python%20notebooks/reinforcement-learning-exercises/DP/Policy%20Evaluation.ipynb#ch0000003?line=0'>1</a>\u001b[0m random_policy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones([env\u001b[39m.\u001b[39mnS, env\u001b[39m.\u001b[39mnA]) \u001b[39m/\u001b[39m env\u001b[39m.\u001b[39mnA\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jacobjun/Python%20notebooks/reinforcement-learning-exercises/DP/Policy%20Evaluation.ipynb#ch0000003?line=1'>2</a>\u001b[0m v \u001b[39m=\u001b[39m policy_eval(random_policy, env)\n",
      "\u001b[1;32m/Users/jacobjun/Python notebooks/reinforcement-learning-exercises/DP/Policy Evaluation.ipynb Cell 3'\u001b[0m in \u001b[0;36mpolicy_eval\u001b[0;34m(policy, env, discount_factor, theta)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobjun/Python%20notebooks/reinforcement-learning-exercises/DP/Policy%20Evaluation.ipynb#ch0000002?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m a, action_prob \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(policy[s]):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobjun/Python%20notebooks/reinforcement-learning-exercises/DP/Policy%20Evaluation.ipynb#ch0000002?line=27'>28</a>\u001b[0m     \u001b[39m# using the lookahead idea from Bellman expectation equation to calculate the current value\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobjun/Python%20notebooks/reinforcement-learning-exercises/DP/Policy%20Evaluation.ipynb#ch0000002?line=28'>29</a>\u001b[0m     \u001b[39m# note that env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobjun/Python%20notebooks/reinforcement-learning-exercises/DP/Policy%20Evaluation.ipynb#ch0000002?line=29'>30</a>\u001b[0m     \u001b[39mfor\u001b[39;00m prob, next_state, reward, done \u001b[39min\u001b[39;00m env\u001b[39m.\u001b[39mP[s][a]:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jacobjun/Python%20notebooks/reinforcement-learning-exercises/DP/Policy%20Evaluation.ipynb#ch0000002?line=30'>31</a>\u001b[0m         new_value \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prob \u001b[39m*\u001b[39m action_prob \u001b[39m*\u001b[39m (reward \u001b[39m+\u001b[39m (discount_factor \u001b[39m*\u001b[39m V[next_state]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobjun/Python%20notebooks/reinforcement-learning-exercises/DP/Policy%20Evaluation.ipynb#ch0000002?line=32'>33</a>\u001b[0m     \u001b[39m# update delta value\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobjun/Python%20notebooks/reinforcement-learning-exercises/DP/Policy%20Evaluation.ipynb#ch0000002?line=33'>34</a>\u001b[0m     delta \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(delta, np\u001b[39m.\u001b[39mabs(new_value \u001b[39m-\u001b[39m V[s]))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "v = policy_eval(random_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 2 decimals\n\n(mismatch 87.5%)\n x: array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n        0.,  0.,  0.])\n y: array([  0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22,\n       -20, -14,   0])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-235f39fb115c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test: Make sure the evaluated policy is what we expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexpected_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_array_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/numpy/testing/utils.py\u001b[0m in \u001b[0;36massert_array_almost_equal\u001b[0;34m(x, y, decimal, err_msg, verbose)\u001b[0m\n\u001b[1;32m    914\u001b[0m     assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n\u001b[1;32m    915\u001b[0m              \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Arrays are not almost equal to %d decimals'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m              precision=decimal)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/numpy/testing/utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision)\u001b[0m\n\u001b[1;32m    735\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[1;32m    736\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 2 decimals\n\n(mismatch 87.5%)\n x: array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n        0.,  0.,  0.])\n y: array([  0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22,\n       -20, -14,   0])"
     ]
    }
   ],
   "source": [
    "# Test: Make sure the evaluated policy is what we expected\n",
    "expected_v = np.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0])\n",
    "np.testing.assert_array_almost_equal(v, expected_v, decimal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('RL-practice': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "a2f77e9e97afd9cbb51f40ef9c588f12e048704560efbd25c32598dbba163ce0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
